{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f47a348-f3e2-4898-b574-def56955a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bb0367-5052-4c1a-be84-de44798c2996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wi1</th>\n",
       "      <th>Wi2</th>\n",
       "      <th>Wi3</th>\n",
       "      <th>Wi5</th>\n",
       "      <th>DC Gain</th>\n",
       "      <th>ft</th>\n",
       "      <th>f3</th>\n",
       "      <th>Vcm</th>\n",
       "      <th>Pdiss</th>\n",
       "      <th>Is4</th>\n",
       "      <th>Gm6</th>\n",
       "      <th>Gm4</th>\n",
       "      <th>Wi4</th>\n",
       "      <th>Asp_1</th>\n",
       "      <th>Asp_2</th>\n",
       "      <th>Asp_3</th>\n",
       "      <th>Asp_4</th>\n",
       "      <th>Asp_5</th>\n",
       "      <th>Abs_Gain</th>\n",
       "      <th>Delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>20.0438</td>\n",
       "      <td>6080000.0</td>\n",
       "      <td>598810.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>54.8</td>\n",
       "      <td>120.0</td>\n",
       "      <td>10.050554</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>20.0816</td>\n",
       "      <td>12100000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>182.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>10.094388</td>\n",
       "      <td>0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>20.1075</td>\n",
       "      <td>13200000.0</td>\n",
       "      <td>1310000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>200.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>10.124533</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>20.4283</td>\n",
       "      <td>12100000.0</td>\n",
       "      <td>1140000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>162.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>10.505458</td>\n",
       "      <td>0.004502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>20.4548</td>\n",
       "      <td>14700000.0</td>\n",
       "      <td>1390000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>200.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>10.537559</td>\n",
       "      <td>0.003491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>71.8244</td>\n",
       "      <td>33600000.0</td>\n",
       "      <td>20855.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>200.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>3901.395691</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>71.8294</td>\n",
       "      <td>32800000.0</td>\n",
       "      <td>20633.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>182.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3903.642162</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>71.8470</td>\n",
       "      <td>33400000.0</td>\n",
       "      <td>20821.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>3911.560033</td>\n",
       "      <td>0.004497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>71.8508</td>\n",
       "      <td>34200000.0</td>\n",
       "      <td>20854.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>200.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3913.271680</td>\n",
       "      <td>0.002490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>71.8829</td>\n",
       "      <td>32800000.0</td>\n",
       "      <td>20675.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3927.760516</td>\n",
       "      <td>0.002490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2044 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Wi1       Wi2       Wi3       Wi5  DC Gain          ft         f3  \\\n",
       "0     0.000001  0.000021  0.000091  0.000060  20.0438   6080000.0   598810.0   \n",
       "1     0.000091  0.000100  0.000011  0.000007  20.0816  12100000.0  1200000.0   \n",
       "2     0.000100  0.000091  0.000011  0.000007  20.1075  13200000.0  1310000.0   \n",
       "3     0.000081  0.000100  0.000011  0.000007  20.4283  12100000.0  1140000.0   \n",
       "4     0.000100  0.000081  0.000011  0.000008  20.4548  14700000.0  1390000.0   \n",
       "...        ...       ...       ...       ...      ...         ...        ...   \n",
       "2039  0.000100  0.000091  0.000100  0.000055  71.8244  33600000.0    20855.9   \n",
       "2040  0.000091  0.000100  0.000100  0.000053  71.8294  32800000.0    20633.1   \n",
       "2041  0.000100  0.000100  0.000091  0.000050  71.8470  33400000.0    20821.4   \n",
       "2042  0.000100  0.000091  0.000091  0.000053  71.8508  34200000.0    20854.2   \n",
       "2043  0.000100  0.000100  0.000100  0.000053  71.8829  32800000.0    20675.9   \n",
       "\n",
       "      Vcm     Pdiss       Is4       Gm6       Gm4       Wi4  Asp_1  Asp_2  \\\n",
       "0     1.6  0.000068  0.000011  0.000329  0.000252  0.000027    2.0   42.0   \n",
       "1     1.6  0.000066  0.000012  0.000655  0.000554  0.000118  182.0  200.0   \n",
       "2     1.6  0.000069  0.000012  0.000716  0.000529  0.000123  200.0  182.0   \n",
       "3     1.6  0.000066  0.000011  0.000652  0.000554  0.000118  162.0  200.0   \n",
       "4     1.6  0.000072  0.000012  0.000795  0.000499  0.000129  200.0  162.0   \n",
       "...   ...       ...       ...       ...       ...       ...    ...    ...   \n",
       "2039  0.8  0.000056  0.000010  0.001820  0.000493  0.000335  200.0  182.0   \n",
       "2040  0.8  0.000055  0.000010  0.001770  0.000517  0.000342  182.0  200.0   \n",
       "2041  0.8  0.000056  0.000010  0.001800  0.000517  0.000349  200.0  200.0   \n",
       "2042  0.8  0.000057  0.000010  0.001850  0.000493  0.000341  200.0  182.0   \n",
       "2043  0.8  0.000055  0.000010  0.001770  0.000517  0.000343  200.0  200.0   \n",
       "\n",
       "      Asp_3  Asp_4  Asp_5     Abs_Gain     Delay  \n",
       "0     182.0   54.8  120.0    10.050554  0.000488  \n",
       "1      22.0  236.0   13.4    10.094388  0.000503  \n",
       "2      22.0  246.0   14.7    10.124533  0.000498  \n",
       "3      22.0  236.0   13.4    10.505458  0.004502  \n",
       "4      22.0  258.0   16.4    10.537559  0.003491  \n",
       "...     ...    ...    ...          ...       ...  \n",
       "2039  200.0  670.0  111.0  3901.395691  0.000493  \n",
       "2040  200.0  684.0  106.0  3903.642162  0.000527  \n",
       "2041  182.0  698.0  101.0  3911.560033  0.004497  \n",
       "2042  182.0  682.0  106.0  3913.271680  0.002490  \n",
       "2043  200.0  686.0  106.0  3927.760516  0.002490  \n",
       "\n",
       "[2044 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"2STAGEOPAMP_DATASET.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13fb99c9-064f-42a5-977b-6ebe0a39f602",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Input Features:\n",
    "\n",
    "DC Gain\n",
    "Unity Gain Frequency (ft)\n",
    "3-dB Frequency (f3)\n",
    "Common Mode Voltage (Vcm)\n",
    "Power Dissipation (Pdiss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86d34301-1256-4139-885b-fdc01060c1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-07"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "0.000001/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04e57621-d891-4823-aa5a-ddfd4926ba8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-07"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.000021/42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b7054011-6630-4b7d-92c5-eca298a0476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-07"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.000100/\t200.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cacdc1e6-1435-4a9b-8dd7-b721664c5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"DC Gain\",\"ft\",\"f3\",\"Vcm\",\"Pdiss\",\"Abs_Gain\",\"Delay\"]\n",
    "labels = ['Wi1', 'Wi2', 'Wi3', 'Wi4', 'Wi5',\"Is4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f29ac8-c9f0-4704-a7f8-fa759947936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DC Gain</th>\n",
       "      <th>ft</th>\n",
       "      <th>f3</th>\n",
       "      <th>Vcm</th>\n",
       "      <th>Pdiss</th>\n",
       "      <th>Abs_Gain</th>\n",
       "      <th>Delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>21.9597</td>\n",
       "      <td>23400000.0</td>\n",
       "      <td>1840000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>12.530979</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>21.9600</td>\n",
       "      <td>12700000.0</td>\n",
       "      <td>1010000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>12.531412</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>22.1025</td>\n",
       "      <td>7760000.0</td>\n",
       "      <td>585552.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>12.738697</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>22.1694</td>\n",
       "      <td>25100000.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>12.837191</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>22.1990</td>\n",
       "      <td>18600000.0</td>\n",
       "      <td>1440000.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>12.881012</td>\n",
       "      <td>0.002510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>51.1962</td>\n",
       "      <td>44400000.0</td>\n",
       "      <td>212842.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>362.919246</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>51.2529</td>\n",
       "      <td>26500000.0</td>\n",
       "      <td>82847.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>365.296070</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>51.5348</td>\n",
       "      <td>39200000.0</td>\n",
       "      <td>191259.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>377.346217</td>\n",
       "      <td>0.002505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>51.7011</td>\n",
       "      <td>47100000.0</td>\n",
       "      <td>185099.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>384.640491</td>\n",
       "      <td>0.000488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>51.8663</td>\n",
       "      <td>49000000.0</td>\n",
       "      <td>163215.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>392.026116</td>\n",
       "      <td>0.004502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1029 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DC Gain          ft         f3  Vcm     Pdiss    Abs_Gain     Delay\n",
       "67    21.9597  23400000.0  1840000.0  1.6  0.000082   12.530979  0.000488\n",
       "68    21.9600  12700000.0  1010000.0  1.6  0.000068   12.531412  0.001504\n",
       "69    22.1025   7760000.0   585552.0  0.8  0.000062   12.738697  0.000488\n",
       "70    22.1694  25100000.0  2000000.0  1.6  0.000101   12.837191  0.001543\n",
       "71    22.1990  18600000.0  1440000.0  1.6  0.000082   12.881012  0.002510\n",
       "...       ...         ...        ...  ...       ...         ...       ...\n",
       "1091  51.1962  44400000.0   212842.0  1.6  0.000082  362.919246  0.000488\n",
       "1092  51.2529  26500000.0    82847.1  1.6  0.000114  365.296070  0.000498\n",
       "1093  51.5348  39200000.0   191259.0  1.6  0.000073  377.346217  0.002505\n",
       "1094  51.7011  47100000.0   185099.0  1.6  0.000098  384.640491  0.000488\n",
       "1095  51.8663  49000000.0   163215.0  0.8  0.000146  392.026116  0.004502\n",
       "\n",
       "[1029 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[features]\n",
    "X[67:1096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507da075-0630-4d1d-ba43-c00d88498f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5edd14-f026-48fe-91da-570c3f21c3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wi1</th>\n",
       "      <th>Wi2</th>\n",
       "      <th>Wi3</th>\n",
       "      <th>Wi4</th>\n",
       "      <th>Wi5</th>\n",
       "      <th>Is4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2044 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Wi1       Wi2       Wi3       Wi4       Wi5       Is4\n",
       "0     0.000001  0.000021  0.000091  0.000027  0.000060  0.000011\n",
       "1     0.000091  0.000100  0.000011  0.000118  0.000007  0.000012\n",
       "2     0.000100  0.000091  0.000011  0.000123  0.000007  0.000012\n",
       "3     0.000081  0.000100  0.000011  0.000118  0.000007  0.000011\n",
       "4     0.000100  0.000081  0.000011  0.000129  0.000008  0.000012\n",
       "...        ...       ...       ...       ...       ...       ...\n",
       "2039  0.000100  0.000091  0.000100  0.000335  0.000055  0.000010\n",
       "2040  0.000091  0.000100  0.000100  0.000342  0.000053  0.000010\n",
       "2041  0.000100  0.000100  0.000091  0.000349  0.000050  0.000010\n",
       "2042  0.000100  0.000091  0.000091  0.000341  0.000053  0.000010\n",
       "2043  0.000100  0.000100  0.000100  0.000343  0.000053  0.000010\n",
       "\n",
       "[2044 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data[labels]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5affdf-239c-4cbf-91c0-b84d1cf22b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06c20d70-a1f5-4639-bf6d-5db8c58af575",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "model_1 = linear\n",
    "model_2 = knn\n",
    "model_3 decision forest\n",
    "model_4= random forest\n",
    "model_5 = GaussianProcessRegressor\n",
    "model_6 =  neural \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a729c75b-91e5-4dee-b05c-5ecf964908b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Importing models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21290667-c2a1-44fa-9083-ad9413942ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = [\"DC Gain\", \"ft\", \"Vcm\", \"Pdiss\", \"Abs_Gain\", \"Delay\"]\n",
    "output_targets = ['Wi1', 'Wi2', 'Wi3', 'Wi4', 'Wi5',\"Is4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c4ffeb-a7df-40d3-9028-5bf0cb9105c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"2STAGEOPAMP_DATASET.csv\")\n",
    "X = df[input_features]  # Features\n",
    "y = df[output_targets]  # Target variables (multi-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11413153-97f4-430b-9561-d282b34efbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: MSE = 0.0000, R² = 0.5520\n",
      "K-Nearest Neighbors: MSE = 0.0000, R² = 0.7559\n",
      "Decision Tree: MSE = 0.0000, R² = 0.8329\n",
      "Random Forest: MSE = 0.0000, R² = 0.9117\n",
      "Gaussian Process: MSE = 0.0000, R² = -532.6794\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature and Target scaling\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "b = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# Dictionary to store models and their performance\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Inverse scaling to original values\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{model_name}: MSE = {mse:.4f}, R² = {r2:.4f}\")\n",
    "    return {\"MSE\": mse, \"R²\": r2}\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "def linear_regression_model():\n",
    "    model = LinearRegression()\n",
    "    return train_and_evaluate_model(model, \"Linear Regression\")\n",
    "\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "def knn_model():\n",
    "    model = KNeighborsRegressor()\n",
    "    return train_and_evaluate_model(model, \"K-Nearest Neighbors\")\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "def decision_tree_model():\n",
    "    model = DecisionTreeRegressor()\n",
    "    return train_and_evaluate_model(model, \"Decision Tree\")\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "def random_forest_model():\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    return train_and_evaluate_model(model, \"Random Forest\")\n",
    "\n",
    "\n",
    "# Gaussian Process\n",
    "def gaussian_process_model():\n",
    "    model = GaussianProcessRegressor()\n",
    "    return train_and_evaluate_model(model, \"Gaussian Process\")\n",
    "\n",
    "# Run all models and collect results\n",
    "results = {\n",
    "    \"Linear Regression\": linear_regression_model(),\n",
    "    \"K-Nearest Neighbors\": knn_model(),\n",
    "    \"Decision Tree\": decision_tree_model(),\n",
    "    \"Random Forest\": random_forest_model(),\n",
    "    \"Gaussian Process\": gaussian_process_model(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60f01767-ff7c-449c-9009-1d627f0553e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.17641144, -1.56703589, -1.10653269, ..., -0.05175014,\n",
       "        -0.80608025,  0.60471494],\n",
       "       [ 0.76744088, -0.24230731,  0.36098448, ...,  2.35961392,\n",
       "         0.10896249,  1.32946791],\n",
       "       [-1.10779308, -1.38157389, -0.74956905, ...,  0.29061106,\n",
       "        -0.80329473, -0.87692562],\n",
       "       ...,\n",
       "       [-0.34095352,  0.04913298, -1.02720743, ..., -0.0390701 ,\n",
       "        -0.72402775, -0.13431797],\n",
       "       [ 1.08052756,  0.61435051,  1.25339357, ..., -0.3349378 ,\n",
       "         0.92598153,  1.36159596],\n",
       "       [ 1.14353716,  0.36706784,  1.25339357, ..., -0.4955517 ,\n",
       "         1.16207755,  1.32589975]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41679d3a-3543-4e29-95d0-36047ce4fbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.8313 - mae: 0.8313\n",
      "Epoch 2/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7095 - mae: 0.7095\n",
      "Epoch 3/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6246 - mae: 0.6246\n",
      "Epoch 4/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5537 - mae: 0.5537\n",
      "Epoch 5/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5349 - mae: 0.5349\n",
      "Epoch 6/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5027 - mae: 0.5027\n",
      "Epoch 7/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4804 - mae: 0.4804\n",
      "Epoch 8/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4720 - mae: 0.4720\n",
      "Epoch 9/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4603 - mae: 0.4603\n",
      "Epoch 10/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4544 - mae: 0.4544\n",
      "Epoch 11/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4523 - mae: 0.4523\n",
      "Epoch 12/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4463 - mae: 0.4463\n",
      "Epoch 13/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4444 - mae: 0.4444\n",
      "Epoch 14/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4134 - mae: 0.4134\n",
      "Epoch 15/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4301 - mae: 0.4301\n",
      "Epoch 16/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4188 - mae: 0.4188\n",
      "Epoch 17/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3971 - mae: 0.3971\n",
      "Epoch 18/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4037 - mae: 0.4037\n",
      "Epoch 19/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3961 - mae: 0.3961\n",
      "Epoch 20/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4035 - mae: 0.4035\n",
      "Epoch 21/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3921 - mae: 0.3921\n",
      "Epoch 22/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3898 - mae: 0.3898\n",
      "Epoch 23/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4020 - mae: 0.4020\n",
      "Epoch 24/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3782 - mae: 0.3782\n",
      "Epoch 25/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3825 - mae: 0.3825\n",
      "Epoch 26/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3829 - mae: 0.3829\n",
      "Epoch 27/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3679 - mae: 0.3679\n",
      "Epoch 28/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3736 - mae: 0.3736\n",
      "Epoch 29/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3677 - mae: 0.3677\n",
      "Epoch 30/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3748 - mae: 0.3748\n",
      "Epoch 31/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3704 - mae: 0.3704\n",
      "Epoch 32/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3687 - mae: 0.3687\n",
      "Epoch 33/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3592 - mae: 0.3592\n",
      "Epoch 34/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3660 - mae: 0.3660\n",
      "Epoch 35/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3650 - mae: 0.3650\n",
      "Epoch 36/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3580 - mae: 0.3580\n",
      "Epoch 37/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3595 - mae: 0.3595\n",
      "Epoch 38/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3579 - mae: 0.3579\n",
      "Epoch 39/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3574 - mae: 0.3574\n",
      "Epoch 40/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3602 - mae: 0.3602\n",
      "Epoch 41/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3478 - mae: 0.3478\n",
      "Epoch 42/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3465 - mae: 0.3465\n",
      "Epoch 43/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3455 - mae: 0.3455\n",
      "Epoch 44/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3529 - mae: 0.3529\n",
      "Epoch 45/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3493 - mae: 0.3493\n",
      "Epoch 46/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3382 - mae: 0.3382\n",
      "Epoch 47/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3382 - mae: 0.3382\n",
      "Epoch 48/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3238 - mae: 0.3238\n",
      "Epoch 49/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3289 - mae: 0.3289\n",
      "Epoch 50/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3232 - mae: 0.3232\n",
      "Epoch 51/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3248 - mae: 0.3248\n",
      "Epoch 52/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3340 - mae: 0.3340\n",
      "Epoch 53/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3309 - mae: 0.3309\n",
      "Epoch 54/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3369 - mae: 0.3369\n",
      "Epoch 55/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3276 - mae: 0.3276\n",
      "Epoch 56/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3169 - mae: 0.3169\n",
      "Epoch 57/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3236 - mae: 0.3236\n",
      "Epoch 58/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3096 - mae: 0.3096\n",
      "Epoch 59/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3181 - mae: 0.3181\n",
      "Epoch 60/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3177 - mae: 0.3177\n",
      "Epoch 61/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3287 - mae: 0.3287\n",
      "Epoch 62/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3135 - mae: 0.3135\n",
      "Epoch 63/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3094 - mae: 0.3094\n",
      "Epoch 64/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3155 - mae: 0.3155\n",
      "Epoch 65/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3309 - mae: 0.3309\n",
      "Epoch 66/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3142 - mae: 0.3142\n",
      "Epoch 67/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3037 - mae: 0.3037\n",
      "Epoch 68/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3074 - mae: 0.3074\n",
      "Epoch 69/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2972 - mae: 0.2972\n",
      "Epoch 70/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3003 - mae: 0.3003\n",
      "Epoch 71/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3128 - mae: 0.3128\n",
      "Epoch 72/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3055 - mae: 0.3055\n",
      "Epoch 73/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3059 - mae: 0.3059\n",
      "Epoch 74/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3074 - mae: 0.3074\n",
      "Epoch 75/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2953 - mae: 0.2953\n",
      "Epoch 76/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2926 - mae: 0.2926\n",
      "Epoch 77/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2966 - mae: 0.2966\n",
      "Epoch 78/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2916 - mae: 0.2916\n",
      "Epoch 79/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2884 - mae: 0.2884\n",
      "Epoch 80/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2966 - mae: 0.2966\n",
      "Epoch 81/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2914 - mae: 0.2914\n",
      "Epoch 82/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2944 - mae: 0.2944\n",
      "Epoch 83/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2951 - mae: 0.2951\n",
      "Epoch 84/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2982 - mae: 0.2982\n",
      "Epoch 85/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2841 - mae: 0.2841\n",
      "Epoch 86/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2958 - mae: 0.2958\n",
      "Epoch 87/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2976 - mae: 0.2976\n",
      "Epoch 88/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2822 - mae: 0.2822\n",
      "Epoch 89/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2757 - mae: 0.2757\n",
      "Epoch 90/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2807 - mae: 0.2807\n",
      "Epoch 91/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2767 - mae: 0.2767\n",
      "Epoch 92/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2749 - mae: 0.2749\n",
      "Epoch 93/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2743 - mae: 0.2743\n",
      "Epoch 94/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2826 - mae: 0.2826\n",
      "Epoch 95/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2639 - mae: 0.2639\n",
      "Epoch 96/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2741 - mae: 0.2741\n",
      "Epoch 97/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2862 - mae: 0.2862\n",
      "Epoch 98/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2872 - mae: 0.2872\n",
      "Epoch 99/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2730 - mae: 0.2730\n",
      "Epoch 100/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2864 - mae: 0.2864\n",
      "Epoch 101/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2790 - mae: 0.2790\n",
      "Epoch 102/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2813 - mae: 0.2813\n",
      "Epoch 103/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2674 - mae: 0.2674\n",
      "Epoch 104/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2769 - mae: 0.2769\n",
      "Epoch 105/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2730 - mae: 0.2730\n",
      "Epoch 106/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2727 - mae: 0.2727\n",
      "Epoch 107/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2718 - mae: 0.2718\n",
      "Epoch 108/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2678 - mae: 0.2678\n",
      "Epoch 109/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2684 - mae: 0.2684\n",
      "Epoch 110/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2681 - mae: 0.2681\n",
      "Epoch 111/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2653 - mae: 0.2653\n",
      "Epoch 112/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2678 - mae: 0.2678\n",
      "Epoch 113/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2693 - mae: 0.2693\n",
      "Epoch 114/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2566 - mae: 0.2566\n",
      "Epoch 115/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2618 - mae: 0.2618\n",
      "Epoch 116/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2647 - mae: 0.2647\n",
      "Epoch 117/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2665 - mae: 0.2665\n",
      "Epoch 118/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2600 - mae: 0.2600\n",
      "Epoch 119/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2599 - mae: 0.2599\n",
      "Epoch 120/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2662 - mae: 0.2662\n",
      "Epoch 121/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2556 - mae: 0.2556\n",
      "Epoch 122/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2588 - mae: 0.2588\n",
      "Epoch 123/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2705 - mae: 0.2705\n",
      "Epoch 124/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2540 - mae: 0.2540\n",
      "Epoch 125/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2493 - mae: 0.2493\n",
      "Epoch 126/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2623 - mae: 0.2623\n",
      "Epoch 127/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2642 - mae: 0.2642\n",
      "Epoch 128/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2555 - mae: 0.2555\n",
      "Epoch 129/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2586 - mae: 0.2586\n",
      "Epoch 130/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2540 - mae: 0.2540\n",
      "Epoch 131/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2443 - mae: 0.2443\n",
      "Epoch 132/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2562 - mae: 0.2562\n",
      "Epoch 133/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2486 - mae: 0.2486\n",
      "Epoch 134/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2510 - mae: 0.2510\n",
      "Epoch 135/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2486 - mae: 0.2486\n",
      "Epoch 136/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2565 - mae: 0.2565\n",
      "Epoch 137/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2535 - mae: 0.2535\n",
      "Epoch 138/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2547 - mae: 0.2547\n",
      "Epoch 139/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2561 - mae: 0.2561\n",
      "Epoch 140/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2491 - mae: 0.2491\n",
      "Epoch 141/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2429 - mae: 0.2429\n",
      "Epoch 142/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2549 - mae: 0.2549\n",
      "Epoch 143/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2514 - mae: 0.2514\n",
      "Epoch 144/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2425 - mae: 0.2425\n",
      "Epoch 145/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2516 - mae: 0.2516\n",
      "Epoch 146/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2512 - mae: 0.2512\n",
      "Epoch 147/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2397 - mae: 0.2397\n",
      "Epoch 148/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2424 - mae: 0.2424\n",
      "Epoch 149/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2411 - mae: 0.2411\n",
      "Epoch 150/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2466 - mae: 0.2466\n",
      "Epoch 151/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2493 - mae: 0.2493\n",
      "Epoch 152/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2457 - mae: 0.2457\n",
      "Epoch 153/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2469 - mae: 0.2469\n",
      "Epoch 154/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2429 - mae: 0.2429\n",
      "Epoch 155/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2436 - mae: 0.2436\n",
      "Epoch 156/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2424 - mae: 0.2424\n",
      "Epoch 157/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2313 - mae: 0.2313\n",
      "Epoch 158/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2468 - mae: 0.2468\n",
      "Epoch 159/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2444 - mae: 0.2444\n",
      "Epoch 160/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2359 - mae: 0.2359\n",
      "Epoch 161/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2448 - mae: 0.2448\n",
      "Epoch 162/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2378 - mae: 0.2378\n",
      "Epoch 163/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2503 - mae: 0.2503\n",
      "Epoch 164/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2424 - mae: 0.2424\n",
      "Epoch 165/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2459 - mae: 0.2459\n",
      "Epoch 166/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2389 - mae: 0.2389\n",
      "Epoch 167/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2382 - mae: 0.2382\n",
      "Epoch 168/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2323 - mae: 0.2323\n",
      "Epoch 169/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2465 - mae: 0.2465\n",
      "Epoch 170/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2335 - mae: 0.2335\n",
      "Epoch 171/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2384 - mae: 0.2384\n",
      "Epoch 172/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2348 - mae: 0.2348\n",
      "Epoch 173/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2361 - mae: 0.2361\n",
      "Epoch 174/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2331 - mae: 0.2331\n",
      "Epoch 175/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2319 - mae: 0.2319\n",
      "Epoch 176/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2290 - mae: 0.2290\n",
      "Epoch 177/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2387 - mae: 0.2387\n",
      "Epoch 178/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2256 - mae: 0.2256\n",
      "Epoch 179/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2403 - mae: 0.2403\n",
      "Epoch 180/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2248 - mae: 0.2248\n",
      "Epoch 181/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2375 - mae: 0.2375\n",
      "Epoch 182/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2356 - mae: 0.2356\n",
      "Epoch 183/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2242 - mae: 0.2242\n",
      "Epoch 184/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2346 - mae: 0.2346\n",
      "Epoch 185/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2261 - mae: 0.2261\n",
      "Epoch 186/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2370 - mae: 0.2370\n",
      "Epoch 187/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2244 - mae: 0.2244\n",
      "Epoch 188/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2332 - mae: 0.2332\n",
      "Epoch 189/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2255 - mae: 0.2255\n",
      "Epoch 190/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2302 - mae: 0.2302\n",
      "Epoch 191/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2296 - mae: 0.2296\n",
      "Epoch 192/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2296 - mae: 0.2296\n",
      "Epoch 193/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2265 - mae: 0.2265\n",
      "Epoch 194/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2346 - mae: 0.2346\n",
      "Epoch 195/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2313 - mae: 0.2313\n",
      "Epoch 196/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2319 - mae: 0.2319\n",
      "Epoch 197/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2183 - mae: 0.2183\n",
      "Epoch 198/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2347 - mae: 0.2347\n",
      "Epoch 199/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2297 - mae: 0.2297\n",
      "Epoch 200/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2264 - mae: 0.2264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d922feab0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "\n",
    "nn_model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train_scaled.shape[1])  \n",
    "])\n",
    "\n",
    "nn_model.compile(loss = tf.keras.losses.mae,\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae'])\n",
    "nn_model.fit(tf.expand_dims(X_train_scaled,axis=-1), y_train_scaled, epochs=200, batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a2313c-3de7-4280-bd69-e7fee6a7b7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Neural Network: MSE = 0.0000, R² = 0.8572\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Neural Network\n",
    "y_pred_nn_scaled = nn_model.predict(X_test_scaled)\n",
    "y_pred_nn = scaler_y.inverse_transform(y_pred_nn_scaled)\n",
    "\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "results[\"Neural Network\"] = {\"MSE\": mse_nn, \"R²\": r2_nn}\n",
    "print(f\"Neural Network: MSE = {mse_nn:.4f}, R² = {r2_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee4d429d-ef14-40d5-b543-0efdac4d81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Custom R² Score function\n",
    "def r2_score_2(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))  # Residual sum of squares\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))  # Total sum of squares\n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())  # Adding epsilon to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22519e5a-c5be-4787-a911-e5e9f419e34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.8133 - r2_score_2: -2.8414e-05\n",
      "Epoch 2/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7250 - r2_score_2: 0.1985\n",
      "Epoch 3/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6450 - r2_score_2: 0.3233\n",
      "Epoch 4/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.6006 - r2_score_2: 0.3949\n",
      "Epoch 5/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5603 - r2_score_2: 0.4442\n",
      "Epoch 6/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5322 - r2_score_2: 0.4682\n",
      "Epoch 7/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5216 - r2_score_2: 0.4765\n",
      "Epoch 8/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4945 - r2_score_2: 0.5142\n",
      "Epoch 9/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4836 - r2_score_2: 0.5208\n",
      "Epoch 10/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4736 - r2_score_2: 0.5152\n",
      "Epoch 11/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4566 - r2_score_2: 0.5483\n",
      "Epoch 12/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4589 - r2_score_2: 0.5546\n",
      "Epoch 13/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4394 - r2_score_2: 0.5891\n",
      "Epoch 14/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4404 - r2_score_2: 0.5818\n",
      "Epoch 15/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4384 - r2_score_2: 0.5837\n",
      "Epoch 16/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4149 - r2_score_2: 0.6070\n",
      "Epoch 17/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4189 - r2_score_2: 0.6081\n",
      "Epoch 18/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4129 - r2_score_2: 0.6106\n",
      "Epoch 19/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4019 - r2_score_2: 0.6297\n",
      "Epoch 20/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4044 - r2_score_2: 0.6180\n",
      "Epoch 21/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4008 - r2_score_2: 0.6402\n",
      "Epoch 22/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3968 - r2_score_2: 0.6358\n",
      "Epoch 23/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3894 - r2_score_2: 0.6366\n",
      "Epoch 24/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3911 - r2_score_2: 0.6517\n",
      "Epoch 25/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3965 - r2_score_2: 0.6355\n",
      "Epoch 26/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3734 - r2_score_2: 0.6762\n",
      "Epoch 27/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3869 - r2_score_2: 0.6549\n",
      "Epoch 28/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3766 - r2_score_2: 0.6653\n",
      "Epoch 29/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3691 - r2_score_2: 0.6859\n",
      "Epoch 30/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3719 - r2_score_2: 0.6752\n",
      "Epoch 31/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3733 - r2_score_2: 0.6741\n",
      "Epoch 32/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3645 - r2_score_2: 0.6885\n",
      "Epoch 33/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3736 - r2_score_2: 0.6745\n",
      "Epoch 34/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3635 - r2_score_2: 0.6923\n",
      "Epoch 35/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3639 - r2_score_2: 0.6922\n",
      "Epoch 36/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3575 - r2_score_2: 0.6989\n",
      "Epoch 37/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3478 - r2_score_2: 0.7087\n",
      "Epoch 38/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3679 - r2_score_2: 0.6782\n",
      "Epoch 39/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3558 - r2_score_2: 0.7009\n",
      "Epoch 40/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3447 - r2_score_2: 0.7201\n",
      "Epoch 41/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3398 - r2_score_2: 0.7333\n",
      "Epoch 42/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3569 - r2_score_2: 0.7017\n",
      "Epoch 43/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3396 - r2_score_2: 0.7190\n",
      "Epoch 44/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3395 - r2_score_2: 0.7263\n",
      "Epoch 45/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3379 - r2_score_2: 0.7267\n",
      "Epoch 46/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3302 - r2_score_2: 0.7398\n",
      "Epoch 47/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3299 - r2_score_2: 0.7377\n",
      "Epoch 48/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3355 - r2_score_2: 0.7309\n",
      "Epoch 49/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3331 - r2_score_2: 0.7358\n",
      "Epoch 50/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3279 - r2_score_2: 0.7435\n",
      "Epoch 51/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3344 - r2_score_2: 0.7384\n",
      "Epoch 52/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3243 - r2_score_2: 0.7476\n",
      "Epoch 53/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3310 - r2_score_2: 0.7462\n",
      "Epoch 54/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3282 - r2_score_2: 0.7468\n",
      "Epoch 55/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3321 - r2_score_2: 0.7403\n",
      "Epoch 56/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3174 - r2_score_2: 0.7534\n",
      "Epoch 57/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3256 - r2_score_2: 0.7473\n",
      "Epoch 58/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3206 - r2_score_2: 0.7581\n",
      "Epoch 59/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3237 - r2_score_2: 0.7554\n",
      "Epoch 60/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3158 - r2_score_2: 0.7598\n",
      "Epoch 61/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3014 - r2_score_2: 0.7769\n",
      "Epoch 62/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3042 - r2_score_2: 0.7812\n",
      "Epoch 63/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3037 - r2_score_2: 0.7826\n",
      "Epoch 64/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3142 - r2_score_2: 0.7674\n",
      "Epoch 65/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2972 - r2_score_2: 0.7888\n",
      "Epoch 66/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2961 - r2_score_2: 0.7973\n",
      "Epoch 67/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3079 - r2_score_2: 0.7752\n",
      "Epoch 68/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2935 - r2_score_2: 0.7969\n",
      "Epoch 69/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2998 - r2_score_2: 0.7858\n",
      "Epoch 70/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2991 - r2_score_2: 0.7858\n",
      "Epoch 71/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2887 - r2_score_2: 0.7960\n",
      "Epoch 72/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2926 - r2_score_2: 0.7899\n",
      "Epoch 73/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2963 - r2_score_2: 0.7977\n",
      "Epoch 74/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2857 - r2_score_2: 0.8018\n",
      "Epoch 75/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2872 - r2_score_2: 0.8048\n",
      "Epoch 76/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2769 - r2_score_2: 0.8153\n",
      "Epoch 77/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2817 - r2_score_2: 0.8126\n",
      "Epoch 78/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2927 - r2_score_2: 0.7977\n",
      "Epoch 79/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2904 - r2_score_2: 0.7867\n",
      "Epoch 80/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2812 - r2_score_2: 0.8119\n",
      "Epoch 81/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2748 - r2_score_2: 0.8144\n",
      "Epoch 82/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2763 - r2_score_2: 0.8165\n",
      "Epoch 83/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2783 - r2_score_2: 0.8122\n",
      "Epoch 84/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2681 - r2_score_2: 0.8209\n",
      "Epoch 85/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2777 - r2_score_2: 0.8184\n",
      "Epoch 86/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2697 - r2_score_2: 0.8199\n",
      "Epoch 87/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2718 - r2_score_2: 0.8181\n",
      "Epoch 88/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2668 - r2_score_2: 0.8172\n",
      "Epoch 89/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2758 - r2_score_2: 0.8162\n",
      "Epoch 90/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2739 - r2_score_2: 0.8183\n",
      "Epoch 91/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2660 - r2_score_2: 0.8240\n",
      "Epoch 92/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2654 - r2_score_2: 0.8269\n",
      "Epoch 93/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2643 - r2_score_2: 0.8263\n",
      "Epoch 94/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2659 - r2_score_2: 0.8243\n",
      "Epoch 95/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2516 - r2_score_2: 0.8432\n",
      "Epoch 96/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2554 - r2_score_2: 0.8410\n",
      "Epoch 97/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2562 - r2_score_2: 0.8365\n",
      "Epoch 98/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2584 - r2_score_2: 0.8281\n",
      "Epoch 99/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2568 - r2_score_2: 0.8354\n",
      "Epoch 100/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2568 - r2_score_2: 0.8317\n",
      "Epoch 101/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2573 - r2_score_2: 0.8254\n",
      "Epoch 102/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2507 - r2_score_2: 0.8459\n",
      "Epoch 103/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2528 - r2_score_2: 0.8470\n",
      "Epoch 104/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2451 - r2_score_2: 0.8440\n",
      "Epoch 105/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2497 - r2_score_2: 0.8447\n",
      "Epoch 106/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2485 - r2_score_2: 0.8453\n",
      "Epoch 107/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2475 - r2_score_2: 0.8458\n",
      "Epoch 108/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2513 - r2_score_2: 0.8395\n",
      "Epoch 109/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2383 - r2_score_2: 0.8522\n",
      "Epoch 110/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2437 - r2_score_2: 0.8577\n",
      "Epoch 111/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2406 - r2_score_2: 0.8434\n",
      "Epoch 112/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2502 - r2_score_2: 0.8437\n",
      "Epoch 113/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2513 - r2_score_2: 0.8326\n",
      "Epoch 114/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2364 - r2_score_2: 0.8530\n",
      "Epoch 115/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2427 - r2_score_2: 0.8512\n",
      "Epoch 116/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2399 - r2_score_2: 0.8518\n",
      "Epoch 117/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2403 - r2_score_2: 0.8480\n",
      "Epoch 118/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2372 - r2_score_2: 0.8548\n",
      "Epoch 119/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2363 - r2_score_2: 0.8548\n",
      "Epoch 120/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2490 - r2_score_2: 0.8472\n",
      "Epoch 121/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2337 - r2_score_2: 0.8612\n",
      "Epoch 122/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2333 - r2_score_2: 0.8562\n",
      "Epoch 123/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2408 - r2_score_2: 0.8531\n",
      "Epoch 124/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2253 - r2_score_2: 0.8721\n",
      "Epoch 125/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2306 - r2_score_2: 0.8618\n",
      "Epoch 126/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2335 - r2_score_2: 0.8591\n",
      "Epoch 127/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2290 - r2_score_2: 0.8640\n",
      "Epoch 128/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2342 - r2_score_2: 0.8592\n",
      "Epoch 129/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2327 - r2_score_2: 0.8627\n",
      "Epoch 130/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2249 - r2_score_2: 0.8622\n",
      "Epoch 131/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2299 - r2_score_2: 0.8665\n",
      "Epoch 132/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2307 - r2_score_2: 0.8576\n",
      "Epoch 133/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2308 - r2_score_2: 0.8567\n",
      "Epoch 134/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2312 - r2_score_2: 0.8560\n",
      "Epoch 135/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2151 - r2_score_2: 0.8762\n",
      "Epoch 136/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2332 - r2_score_2: 0.8530\n",
      "Epoch 137/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2272 - r2_score_2: 0.8621\n",
      "Epoch 138/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2289 - r2_score_2: 0.8592\n",
      "Epoch 139/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2244 - r2_score_2: 0.8681\n",
      "Epoch 140/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2272 - r2_score_2: 0.8575\n",
      "Epoch 141/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2310 - r2_score_2: 0.8574\n",
      "Epoch 142/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2132 - r2_score_2: 0.8793\n",
      "Epoch 143/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2228 - r2_score_2: 0.8677\n",
      "Epoch 144/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2243 - r2_score_2: 0.8628\n",
      "Epoch 145/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2276 - r2_score_2: 0.8610\n",
      "Epoch 146/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2193 - r2_score_2: 0.8793\n",
      "Epoch 147/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2106 - r2_score_2: 0.8780\n",
      "Epoch 148/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2257 - r2_score_2: 0.8638\n",
      "Epoch 149/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2137 - r2_score_2: 0.8780\n",
      "Epoch 150/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2226 - r2_score_2: 0.8660\n",
      "Epoch 151/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2062 - r2_score_2: 0.8841\n",
      "Epoch 152/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2154 - r2_score_2: 0.8698\n",
      "Epoch 153/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2126 - r2_score_2: 0.8810\n",
      "Epoch 154/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2085 - r2_score_2: 0.8852\n",
      "Epoch 155/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2112 - r2_score_2: 0.8806\n",
      "Epoch 156/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2143 - r2_score_2: 0.8753\n",
      "Epoch 157/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2126 - r2_score_2: 0.8730\n",
      "Epoch 158/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2175 - r2_score_2: 0.8720\n",
      "Epoch 159/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2141 - r2_score_2: 0.8763\n",
      "Epoch 160/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2129 - r2_score_2: 0.8779\n",
      "Epoch 161/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2077 - r2_score_2: 0.8821\n",
      "Epoch 162/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2149 - r2_score_2: 0.8805\n",
      "Epoch 163/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2100 - r2_score_2: 0.8793\n",
      "Epoch 164/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2095 - r2_score_2: 0.8853\n",
      "Epoch 165/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2009 - r2_score_2: 0.8896\n",
      "Epoch 166/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2055 - r2_score_2: 0.8828\n",
      "Epoch 167/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2045 - r2_score_2: 0.8859\n",
      "Epoch 168/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2067 - r2_score_2: 0.8866\n",
      "Epoch 169/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2115 - r2_score_2: 0.8788\n",
      "Epoch 170/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2001 - r2_score_2: 0.8900\n",
      "Epoch 171/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2031 - r2_score_2: 0.8862\n",
      "Epoch 172/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2026 - r2_score_2: 0.8770\n",
      "Epoch 173/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1962 - r2_score_2: 0.8896\n",
      "Epoch 174/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1931 - r2_score_2: 0.8933\n",
      "Epoch 175/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2048 - r2_score_2: 0.8811\n",
      "Epoch 176/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2033 - r2_score_2: 0.8850\n",
      "Epoch 177/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2054 - r2_score_2: 0.8802\n",
      "Epoch 178/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1959 - r2_score_2: 0.8945\n",
      "Epoch 179/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1968 - r2_score_2: 0.8904\n",
      "Epoch 180/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2083 - r2_score_2: 0.8783\n",
      "Epoch 181/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2155 - r2_score_2: 0.8638\n",
      "Epoch 182/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2028 - r2_score_2: 0.8850\n",
      "Epoch 183/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1946 - r2_score_2: 0.8970\n",
      "Epoch 184/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1942 - r2_score_2: 0.8950\n",
      "Epoch 185/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2020 - r2_score_2: 0.8851\n",
      "Epoch 186/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2042 - r2_score_2: 0.8861\n",
      "Epoch 187/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1932 - r2_score_2: 0.8914\n",
      "Epoch 188/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1992 - r2_score_2: 0.8880\n",
      "Epoch 189/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1978 - r2_score_2: 0.8916\n",
      "Epoch 190/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1976 - r2_score_2: 0.8939\n",
      "Epoch 191/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1912 - r2_score_2: 0.8925\n",
      "Epoch 192/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1984 - r2_score_2: 0.8839\n",
      "Epoch 193/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1827 - r2_score_2: 0.9086\n",
      "Epoch 194/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2013 - r2_score_2: 0.8820\n",
      "Epoch 195/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1941 - r2_score_2: 0.8869\n",
      "Epoch 196/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1855 - r2_score_2: 0.8986\n",
      "Epoch 197/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1884 - r2_score_2: 0.8966\n",
      "Epoch 198/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1977 - r2_score_2: 0.8898\n",
      "Epoch 199/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2035 - r2_score_2: 0.8814\n",
      "Epoch 200/200\n",
      "\u001b[1m164/164\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1902 - r2_score_2: 0.8957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d94bebf80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_2 = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train_scaled.shape[1])  # Output layer for multi-output\n",
    "])\n",
    "\n",
    "nn_model_2.compile(\n",
    "    loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[r2_score_2]  # Using the custom R² score\n",
    ")\n",
    "\n",
    "nn_model_2.fit(tf.expand_dims(X_train_scaled,axis=-1), y_train_scaled, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c13b834-b968-46de-ae0a-763c2b2aa68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Neural Network: MSE = 0.0000, R² = 0.8683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make Predictions\n",
    "y_pred_nn_scaled_2 = nn_model_2.predict(X_test_scaled)\n",
    "\n",
    "# Ensure predictions are in NumPy array format\n",
    "y_pred_nn_scaled_2 = y_pred_nn_scaled_2 if isinstance(y_pred_nn_scaled_2, np.ndarray) else y_pred_nn_scaled_2.values\n",
    "\n",
    "# Inverse Transform the scaled predictions\n",
    "y_pred_nn_2 = scaler_y.inverse_transform(y_pred_nn_scaled_2)\n",
    "\n",
    "# Ensure y_test and y_pred_nn_2 are NumPy arrays for metric calculations\n",
    "y_test_array = y_test.values if isinstance(y_test, pd.DataFrame) else y_test\n",
    "y_pred_array = y_pred_nn_2 if isinstance(y_pred_nn_2, np.ndarray) else y_pred_nn_2.values\n",
    "\n",
    "# Calculate Metrics\n",
    "mse_nn_2 = mean_squared_error(y_test_array, y_pred_array)\n",
    "r2_nn_2 = r2_score(y_test_array, y_pred_array)\n",
    "\n",
    "# Store Results\n",
    "results[\"Neural Network\"] = {\"MSE\": mse_nn_2, \"R²\": r2_nn_2}\n",
    "print(f\"Neural Network: MSE = {mse_nn_2:.4f}, R² = {r2_nn_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f06fdfb-878b-45ae-af95-49dc5a6dd448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.18254304921217"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero error by replacing zeros with a small number\n",
    "    epsilon = 1e-10\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "    return mape\n",
    "    \n",
    "mape_score = mean_absolute_percentage_error(y_test, y_pred_nn)\n",
    "mape_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1c45493-51dd-49ec-9e6c-8972c970bc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.000368762095512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_score_2 = mean_absolute_percentage_error(y_test, y_pred_nn_2)\n",
    "mape_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dc49392-1215-4ed0-8fe3-7c493335de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trail_scaled = scaler_X.transform([[21.9567,23400000.0,1840000.0,1.6,0.000082,12.530979,0.000488]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c72b08-d0da-4d3f-bed0-323b4fb1904b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c5450-a1d1-4719-a0b2-27ac1cca7c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
